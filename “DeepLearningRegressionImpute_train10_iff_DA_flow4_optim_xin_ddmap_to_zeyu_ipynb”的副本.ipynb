{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“DeepLearningRegressionImpute_train10_iff_DA_flow4_optim_xin_ddmap_to_zeyu.ipynb”的副本",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOnjDr5uyhEm",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hranxUvMyoJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmq9WnX9O8jW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "1d31b3c6-d918-476a-f406-f387c4866492"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on 12-05-2019\n",
        "\n",
        "@author: Xin Huang\n",
        "\"\"\"\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "# from keras.optimizers import RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# data = np.load('Sat_data_small.npz')\n",
        "# data = np.load('/content/content/My Drive/Colab Notebooks/Sat_data_small.npz')\n",
        "# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/train7.npz')\n",
        "data = np.load('/content/drive/My Drive/Colab Notebooks/kddworkshop/fulldata/train10.npz')\n",
        "# data = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/chenxi/testing_10_days.npz')\n",
        "\n",
        "# passive = 1\n",
        "#load common data\n",
        "latlon = data['latlon']\n",
        "iff = data['iff']\n",
        "\n",
        "X_v = data['viirs']\n",
        "Y_v = data['label']\n",
        "print ('X_v shape:');\n",
        "print (X_v.shape);\n",
        "\n",
        "X_c = data['calipso']\n",
        "Y_c = data['label']\n",
        "print ('X_c shape:');\n",
        "print (X_c.shape);\n",
        "\n",
        "inds_v,vals_v = np.where(Y_v>0)\n",
        "Y_v = Y_v[inds_v]\n",
        "X_v = X_v[inds_v]\n",
        "print ('X_v')\n",
        "print (X_v)\n",
        "\n",
        "inds_c,vals_c = np.where(Y_c>0)\n",
        "Y_c = Y_c[inds_c]\n",
        "X_c = X_c[inds_c]\n",
        "print ('X_c')\n",
        "print (X_c)\n",
        "\n",
        "# process common data\n",
        "Latlon = latlon[inds_v]\n",
        "Iff = iff[inds_v]\n",
        "\n",
        "print('original X_v: ', X_v.shape)\n",
        "rows = np.where((X_v[:,0] >= 0) & (X_v[:,0] <= 83) & (X_v[:,15] > 100) & (X_v[:,15] < 400) & (X_v[:,16] > 100) & (X_v[:,16] < 400) & (X_v[:,17] > 100) & (X_v[:,17] < 400) & (X_v[:,18] > 100) & (X_v[:,18] < 400) & (X_v[:,19] > 100) & (X_v[:,19] < 400) & (X_v[:,10] > 0))\n",
        "print(\"rows:\", rows)\n",
        "print(\"rows.shape:\", len(rows))\n",
        "\n",
        "Latlon = Latlon[rows]\n",
        "Iff = Iff[rows]\n",
        "\n",
        "Y_v = Y_v[rows]\n",
        "X_v = X_v[rows]\n",
        "\n",
        "Y_c = Y_c[rows]\n",
        "X_c = X_c[rows]\n",
        "\n",
        "print('after SZA X_v: ', X_v.shape)\n",
        "print('after SZA X_c: ', X_c.shape)\n",
        "\n",
        "#concanate common data\n",
        "# X_v = np.concatenate((X_v, Latlon, Iff), axis=1)\n",
        "X_c = np.concatenate((X_c, Latlon, Iff), axis=1)\n",
        "print (X_v.shape)\n",
        "print (X_c.shape)\n",
        "\n",
        "X_v = np.nan_to_num(X_v)\n",
        "X_c = np.nan_to_num(X_c)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X_v shape:\n",
            "(2099451, 20)\n",
            "X_c shape:\n",
            "(2099451, 25)\n",
            "X_v\n",
            "[[ 97.43999481 -55.29999924   2.52999997 ... 246.61909485 246.91960144\n",
            "  246.64631653]\n",
            " [ 97.43999481 -55.2899971    2.51999998 ... 247.22109985 247.5663147\n",
            "  247.28843689]\n",
            " [ 97.40999603 -55.22999954   2.5        ... 248.27958679 248.7721405\n",
            "  248.57876587]\n",
            " ...\n",
            " [ 94.68000031 -62.64999771   2.12999988 ... 228.96557617 228.62413025\n",
            "  227.59910583]\n",
            " [ 94.68000031 -62.62999725   2.12999988 ... 229.51361084 229.17892456\n",
            "  227.92941284]\n",
            " [ 94.66999817 -62.6099968    2.12999988 ... 230.20637512 230.03952026\n",
            "  228.89453125]]\n",
            "X_c\n",
            "[[ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  2.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " ...\n",
            " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]\n",
            " [ 1.000e+00  1.000e+00  0.000e+00 ...        nan -9.999e+03        nan]]\n",
            "original X_v:  (1667850, 20)\n",
            "rows: (array([    402,     403,     404, ..., 1666325, 1666326, 1666327]),)\n",
            "rows.shape: 1\n",
            "after SZA X_v:  (704800, 20)\n",
            "after SZA X_c:  (704800, 25)\n",
            "(704800, 20)\n",
            "(704800, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtFEmQpsPM6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a4cba2cc-fbb3-49aa-b927-7d86194cf9db"
      },
      "source": [
        "# combine data and split latter to define ground truth for MLR\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "n1=20\n",
        "n2=25\n",
        "X=np.concatenate((X_v, X_c), axis=1)\n",
        "Y=Y_v\n",
        "print (X.shape)\n",
        "print (Y_v)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(X, Y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=Y)\n",
        "\n",
        "# x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp,\n",
        "#                                                     test_size=0.5,\n",
        "#                                                     random_state=0,\n",
        "#                                                     stratify=y_temp)\n",
        "\n",
        "# feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "x_train=sc_X.fit_transform(x_train)\n",
        "x_valid=sc_X.transform(x_valid)\n",
        "# x_test=sc_X.fit_transform(x_test)\n",
        "\n",
        "x_train_v = x_train[:, 0:20]\n",
        "x_train_c = x_train[:, 20:45]\n",
        "x_train_comm = x_train[:, 45:51]\n",
        "x_train_src = x_train[:, 20:51]\n",
        "\n",
        "print(x_train_v.shape)\n",
        "print(x_train_c.shape)\n",
        "print(x_train_comm.shape)\n",
        "print(x_train_src.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "x_valid_v = x_valid[:, 0:20]\n",
        "x_valid_c = x_valid[:, 20:45]\n",
        "x_valid_comm = x_valid[:, 45:51]\n",
        "\n",
        "print(x_valid_v.shape)\n",
        "print(x_valid_c.shape)\n",
        "print(x_valid_comm.shape)\n",
        "\n",
        "# x_test_v = x_test[:, 0:20]\n",
        "# x_test_c = x_test[:, 20:45]\n",
        "# x_test_comm = x_test[:, 45:51]\n",
        "\n",
        "# print(x_test_v.shape)\n",
        "# print(x_test_c.shape)\n",
        "# print(x_test_comm.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(704800, 51)\n",
            "[[2]\n",
            " [2]\n",
            " [2]\n",
            " ...\n",
            " [3]\n",
            " [3]\n",
            " [3]]\n",
            "(493360, 20)\n",
            "(493360, 25)\n",
            "(493360, 6)\n",
            "(493360, 31)\n",
            "(493360, 1)\n",
            "(211440, 20)\n",
            "(211440, 25)\n",
            "(211440, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT2iV-MHXVLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2dd57d43-ac22-4451-ae04-27559d01e435"
      },
      "source": [
        "# x_train_c_pt = model_reg.predict(x_train_v)\n",
        "# # x_test_c_pt = model_reg.predict(x_test_v)\n",
        "# x_valid_c_pt = model_reg.predict(x_valid_v)\n",
        "x_train_c_pt = x_train_v\n",
        "x_valid_c_pt = x_valid_v\n",
        "print(x_train_c_pt.shape)\n",
        "# print(x_test_c_pt.shape)\n",
        "print(x_valid_c_pt.shape)\n",
        "\n",
        "# DLR imputed target domain\n",
        "x_train_pt = np.concatenate((x_train_c_pt, x_train_comm),axis=1)\n",
        "\n",
        "print(x_train_pt.shape)\n",
        "\n",
        "# x_test_pt = np.concatenate((x_test_c_pt, x_test_comm),axis=1)\n",
        "\n",
        "# print(x_test_pt.shape)\n",
        "\n",
        "x_valid_pt = np.concatenate((x_valid_c_pt, x_valid_comm),axis=1)\n",
        "\n",
        "print(x_valid_pt.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(493360, 20)\n",
            "(211440, 20)\n",
            "(493360, 26)\n",
            "(211440, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu_o5k_6yxT2",
        "colab_type": "text"
      },
      "source": [
        "## Current Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMSGjBnV2e8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6b1c1813-fe07-492c-a51b-1f4a09e3de32"
      },
      "source": [
        "data_test = np.load('/content/drive/My Drive/Colab Notebooks/kddworkshop/fulldata/test_138_day.npz')\n",
        "\n",
        "passive =1\n",
        "\n",
        "#load common data\n",
        "latlon_test = data_test['latlon']\n",
        "iff_test = data_test['iff']\n",
        "\n",
        "# if passive ==1:\n",
        "x_t_test = data_test['viirs']\n",
        "y_t_test = data_test['label']\n",
        "# else:\n",
        "x_s_test = data_test['calipso']\n",
        "y_s_test = data_test['label']\n",
        "    \n",
        "inds_test,vals_test = np.where(y_t_test>0)\n",
        "\n",
        "# process common data\n",
        "Latlon_test = latlon_test[inds_test]\n",
        "Iff_test = iff_test[inds_test]\n",
        "\n",
        "Y_t_test = y_t_test[inds_test]\n",
        "X_t_test = x_t_test[inds_test]\n",
        "\n",
        "Y_s_test = y_s_test[inds_test]\n",
        "X_s_test = x_s_test[inds_test]\n",
        "\n",
        "# 0 =< SZA <= 83\n",
        "print('original X_t_test: ', X_t_test.shape)\n",
        "rows_test = np.where((X_t_test[:,0] >= 0) & (X_t_test[:,0] <= 83) & (X_t_test[:,15] > 100) & (X_t_test[:,15] < 400) & (X_t_test[:,16] > 100) & (X_t_test[:,16] < 400) & (X_t_test[:,17] > 100) & (X_t_test[:,17] < 400) & (X_t_test[:,18] > 100) & (X_t_test[:,18] < 400) & (X_t_test[:,19] > 100) & (X_t_test[:,19] < 400) & (X_t_test[:,10] > 0))\n",
        "print(\"rows_test:\", rows_test)\n",
        "print(\"rows_test.shape:\", len(rows_test))\n",
        "\n",
        "Latlon_test = Latlon_test[rows_test]\n",
        "Iff_test = Iff_test[rows_test]\n",
        "\n",
        "Y_t_test = Y_t_test[rows_test]\n",
        "X_t_test = X_t_test[rows_test]\n",
        "\n",
        "Y_s_test = Y_s_test[rows_test]\n",
        "X_s_test = X_s_test[rows_test]\n",
        "\n",
        "X_s_test = np.nan_to_num(X_s_test)\n",
        "X_t_test = np.nan_to_num(X_t_test)\n",
        "\n",
        "print('after SZA X_t_test: ', X_t_test.shape)\n",
        "print('after SZA X_s_test: ', X_s_test.shape)\n",
        "\n",
        "# pca = decomposition.PCA(n_components=20)\n",
        "# pca.fit(X_s_test)\n",
        "# X_s_test = pca.transform(X_s_test)\n",
        "# print (X_s_test.shape)\n",
        "\n",
        "#concanate common data\n",
        "# X_t_test = np.concatenate((X_t_test, Latlon_test, Iff_test), axis=1)\n",
        "X_s_test = np.concatenate((X_s_test, Latlon_test, Iff_test), axis=1)\n",
        "\n",
        "print (X_s_test.shape)\n",
        "print (X_t_test.shape)\n",
        "\n",
        "X_test=np.concatenate((X_t_test, X_s_test), axis=1)\n",
        "\n",
        "# scaler_t = StandardScaler()\n",
        "# scaler_t.fit(X_t_test)\n",
        "# X_t_test = scaler_t.transform(X_t_test)\n",
        "\n",
        "# scaler_s = StandardScaler()\n",
        "# scaler_s.fit(X_s_test)\n",
        "# X_s_test= scaler_s.transform(X_s_test)\n",
        "\n",
        "x_test2=sc_X.transform(X_test)\n",
        "\n",
        "# x_train_v = x_train[:, 0:20]\n",
        "# x_train_c = x_train[:, 20:45]\n",
        "# x_train_comm = x_train[:, 45:51]\n",
        "# print(x_train_v.shape)\n",
        "# print(x_train_c.shape)\n",
        "# print(x_train_comm.shape)\n",
        "\n",
        "# x_valid_v = x_valid[:, 0:20]\n",
        "# x_valid_c = x_valid[:, 20:45]\n",
        "# x_valid_comm = x_valid[:, 45:51]\n",
        "\n",
        "# print(x_valid_v.shape)\n",
        "# print(x_valid_c.shape)\n",
        "# print(x_valid_comm.shape)\n",
        "\n",
        "X_t_test = x_test2[:, 0:20]\n",
        "x_test_c2 = x_test2[:, 20:45]\n",
        "x_test_comm2 = x_test2[:, 45:51]\n",
        "\n",
        "\n",
        "# DLR imputed target domain\n",
        "# x_test_t_pt = model_reg.predict(X_t_test)\n",
        "x_test_t_pt = X_t_test\n",
        "print(x_test_t_pt.shape)\n",
        "\n",
        "x_test_pt_test = np.concatenate((x_test_t_pt, x_test_comm2),axis=1)\n",
        "print(x_test_pt_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original X_t_test:  (144703, 20)\n",
            "rows_test: (array([  1502,   1503,   1504, ..., 144700, 144701, 144702]),)\n",
            "rows_test.shape: 1\n",
            "after SZA X_t_test:  (60487, 20)\n",
            "after SZA X_s_test:  (60487, 25)\n",
            "(60487, 31)\n",
            "(60487, 20)\n",
            "(60487, 20)\n",
            "(60487, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEc1f2OVy1Uq",
        "colab_type": "text"
      },
      "source": [
        "## Distributed Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8sGG5gBxdan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYraTkSdKEDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.multiprocessing import Process\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "    \n",
        "\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utONCZt4ziS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Partition(object):\n",
        "\n",
        "    def __init__(self, data, index):\n",
        "        self.data = data\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_idx = self.index[index]\n",
        "        return self.data[data_idx]\n",
        "\n",
        "\n",
        "class DataPartitioner(object):\n",
        "\n",
        "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
        "        self.data = data\n",
        "        self.partitions = []\n",
        "        #rng = Random()\n",
        "        #rng.seed(seed)\n",
        "        data_len = len(data)\n",
        "        indexes = [x for x in range(0, data_len)]\n",
        "        random.shuffle(indexes)\n",
        "\n",
        "        for frac in sizes:\n",
        "            part_len = int(frac * data_len)\n",
        "            self.partitions.append(indexes[0:part_len])\n",
        "            indexes = indexes[part_len:]\n",
        "\n",
        "    def use(self, partition):\n",
        "        return Partition(self.data, self.partitions[partition])\n",
        "\n",
        "\n",
        "def partition_dataset():\n",
        "    dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "    size = dist.get_world_size()\n",
        "    bsz = 128 / float(size)\n",
        "    partition_sizes = [1.0 / size for _ in range(size)]\n",
        "    partition = DataPartitioner(dataset, partition_sizes)\n",
        "    partition = partition.use(dist.get_rank())\n",
        "    train_set = torch.utils.data.DataLoader(partition,\n",
        "                                         batch_size=int(bsz),\n",
        "                                         shuffle=True)\n",
        "    return train_set, bsz\n",
        "\n",
        "def run(rank, size):\n",
        "    print(\"entry process\")\n",
        "    torch.manual_seed(1234)\n",
        "    train_set, bsz = partition_dataset()\n",
        "    model = Net()\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=0.01, momentum=0.5)\n",
        "\n",
        "    num_batches = int(len(train_set.dataset) / float(bsz))\n",
        "    for epoch in range(10):\n",
        "        epoch_loss = 0.0\n",
        "        for data, target in train_set:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            average_gradients(model)\n",
        "            optimizer.step()\n",
        "        print('Rank ', dist.get_rank(), ', epoch ',\n",
        "              epoch, ': ', epoch_loss / num_batches)\n",
        "        \n",
        "\"\"\" Gradient averaging. \"\"\"\n",
        "def average_gradients(model):\n",
        "    size = float(dist.get_world_size())\n",
        "    for param in model.parameters():\n",
        "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
        "        param.grad.data /= size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I4mgFBC9BoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "ed00100a-25e2-4d19-f119-bd5422df57a5"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    size = 2\n",
        "    processes = []\n",
        "    for rank in range(size):\n",
        "        p = Process(target=init_process, args=(rank, size, run))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "entry process\n",
            "entry process\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:102: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:102: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rank  0 , epoch  0 :  0.5354156195321399\n",
            "Rank  1 , epoch  0 :  0.5393918303725047\n",
            "Rank  0 , epoch  1 :  0.2646243585926345\n",
            "Rank  1 , epoch  1 :  0.2708705674188259\n",
            "Rank  1 , epoch  2 :  0.21161150828035724\n",
            "Rank  0 , epoch  2 :  0.21219196488969347\n",
            "Rank  0 , epoch  3 :  0.17467579427851826\n",
            "Rank  1 , epoch  3 :  0.17390869611587662\n",
            "Rank  0 , epoch  4 :  0.14208871940883178\n",
            "Rank  1 , epoch  4 :  0.14463572166302902\n",
            "Rank  1 , epoch  5 :  0.12551722234576687\n",
            "Rank  0 , epoch  5 :  0.12407460037825836\n",
            "Rank  0 , epoch  6 :  0.10556799155843054\n",
            "Rank  1 , epoch  6 :  0.10775171365572983\n",
            "Rank  1 , epoch  7 :  0.09771391523715395\n",
            "Rank  0 , epoch  7 :  0.09259839742205656\n",
            "Rank  1 , epoch  8 :  0.08604959468556266\n",
            "Rank  0 , epoch  8 :  0.08478115303402083\n",
            "Rank  1 , epoch  9 :  0.07996956349955474\n",
            "Rank  0 , epoch  9 :  0.07919284035989012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU3Zmhsu_L4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "2ee4f136-42a8-4467-d481-1fcb10d5bf80"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-212d09bfcaa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mProcessContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ProcessContext' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEo6ess-aRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fe27eff1-bd09-4f6f-c99c-ecaf902e7a8f"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13963965431849344362\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 7309660458367090166\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR47th7q-iyg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11497a1c-d35e-46a3-a016-fa81bda57c6d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bvBgNoXy8Ik",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvcysY7RaZLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the Correlation based DA\n",
        "# pytorch mlp for multiclass classification\n",
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import Module\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import BatchNorm1d\n",
        "from torch.optim import SGD,RMSprop,Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import MSELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import torch\n",
        "\n",
        "\n",
        "n_epochs = 300\n",
        "lambda_ = 0.0001\n",
        "lambda_l2 = 0.1\n",
        "lambda_class = 100\n",
        "# NUM = 31\n",
        "NUM = 26\n",
        "DIFFERECE_COL = 5\n",
        "BATCH_SIZE = 204800\n",
        "\n",
        "# dataset definition\n",
        "class CSVDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, X1, Y1):\n",
        "        # load the csv file as a dataframe\n",
        "        self.X=X1\n",
        "        self.y=Y1\n",
        "        print(\"self.X before fit_transform\")\n",
        "        print(self.X)\n",
        "        print(\"self.y before fit_transform\")\n",
        "        print(self.y)\n",
        "        self.X = self.X.astype('float32')\n",
        "        # label encode target and ensure the values are floats\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\n",
        "        print(\"self.X before fit_transform\")\n",
        "        print(self.X)\n",
        "        print(\"self.y after fit_transform\")\n",
        "        print(self.y)\n",
        "\n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "\n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "def CORAL(src,tgt):\n",
        "    d = src.size(1)\n",
        "    src_c = coral(src)\n",
        "    tgt_c = coral(tgt)\n",
        "\n",
        "    loss = torch.sum(torch.mul((src_c-tgt_c),(src_c-tgt_c)))\n",
        "    loss = loss/(4*d*d)\n",
        "    return loss\n",
        "\n",
        "def LOG_CORAL(src,tgt):\n",
        "    d = src.size(1)\n",
        "    src_c = coral(src)\n",
        "    tgt_c = coral(tgt)\n",
        "    src_vals, src_vecs = torch.symeig(src_c,eigenvectors = True)\n",
        "    tgt_vals, tgt_vecs = torch.symeig(tgt_c,eigenvectors = True)\n",
        "    src_cc = torch.mm(src_vecs,torch.mm(torch.diag(torch.log(src_vals)),src_vecs.t()))\n",
        "    tgt_cc = torch.mm(tgt_vecs,torch.mm(torch.diag(torch.log(tgt_vals)),tgt_vecs.t()))\n",
        "    loss = torch.sum(torch.mul((src_cc - tgt_cc), (src_cc - tgt_cc)))\n",
        "    loss = loss / (4 * d * d)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def coral(data):\n",
        "    n = data.size(0)\n",
        "    id_row = torch.ones(n).resize(1,n)\n",
        "    if torch.cuda.is_available():\n",
        "        id_row = id_row.cuda()\n",
        "    sum_column = torch.mm(id_row,data)\n",
        "    mean_column = torch.div(sum_column,n)\n",
        "    mean_mean = torch.mm(mean_column.t(),mean_column)\n",
        "    d_d = torch.mm(data.t(),data)\n",
        "    coral_result = torch.add(d_d,(-1*mean_mean))*1.0/(n-1)\n",
        "    return coral_result\n",
        "\n",
        "class Deep_coral(Module):\n",
        "    def __init__(self,num_classes = 6):\n",
        "        super(Deep_coral,self).__init__()\n",
        "        self.ddm = DDM(n_inputs=NUM,n_outputs=NUM+DIFFERECE_COL)\n",
        "        self.feature = MLP(n_inputs=NUM+DIFFERECE_COL)\n",
        "        self.fc = Linear(64,num_classes)\n",
        "        xavier_uniform_(self.fc.weight)\n",
        "        #  initial layer\n",
        "        # self.init_layer = Linear(NUM+5, NUM)\n",
        "        # xavier_uniform_(self.init_layer.weight)\n",
        "        # self.act3 = Softmax(dim=1)\n",
        "        # self.fc.weight.data.normal_(0,0.005)# initialization\n",
        "\n",
        "    def forward(self,src,tgt):\n",
        "        src = self.feature(src)\n",
        "        src = self.fc(src)\n",
        "        # output layer\n",
        "        # src = self.act3(src)\n",
        "        # tgt = self.init_layer(tgt)\n",
        "        dmval = self.ddm(tgt)\n",
        "        tgt = self.feature(dmval)\n",
        "        tgt = self.fc(tgt)\n",
        "        # tgt = self.act3(tgt)\n",
        "        return src,tgt,dmval\n",
        "\n",
        "# DDM model definition\n",
        "class DDM(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs=NUM, n_outputs = NUM+DIFFERECE_COL):\n",
        "        super(DDM, self).__init__()\n",
        "        # # input to very beginning hidden layer\n",
        "        self.hidden = Linear(n_inputs, 256)\n",
        "        kaiming_uniform_(self.hidden.weight, nonlinearity='relu')\n",
        "        self.act = ReLU()\n",
        "        # input to beginning hidden layer\n",
        "        self.hidden0 = Linear(256, 256)\n",
        "        kaiming_uniform_(self.hidden0.weight, nonlinearity='relu')\n",
        "        self.act0 = ReLU()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(256, 256)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(256, 128)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # third hidden layer\n",
        "        self.hidden3 = Linear(128, 64)\n",
        "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
        "        self.act3 = ReLU()\n",
        "        # 4th hidden layer\n",
        "        self.hidden4 = Linear(64, n_outputs)\n",
        "        kaiming_uniform_(self.hidden4.weight, nonlinearity='relu')\n",
        "        self.act4 = ReLU()\n",
        "        # # third hidden layer and output\n",
        "        # self.hidden3 = Linear(64, 6)\n",
        "        # xavier_uniform_(self.hidden3.weight)\n",
        "        # self.act3 = Softmax(dim=1)\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "        self.batchnorm = BatchNorm1d(256)\n",
        "        self.batchnorm0 = BatchNorm1d(256)\n",
        "        self.batchnorm1 = BatchNorm1d(256)\n",
        "        self.batchnorm2 = BatchNorm1d(128)\n",
        "        self.batchnorm3 = BatchNorm1d(64)\n",
        "        self.batchnorm4 = BatchNorm1d(n_outputs)\n",
        "\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to very first hidden layer\n",
        "        X = self.hidden(X)\n",
        "        X = self.batchnorm(X)\n",
        "        X = self.act(X)\n",
        "        X = self.dropout(X)\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden0(X)\n",
        "        X = self.batchnorm0(X)\n",
        "        X = self.act0(X)\n",
        "        X = self.dropout(X)\n",
        "        # input to second hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.batchnorm1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.dropout(X)\n",
        "        # third hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.batchnorm2(X)\n",
        "        X = self.act2(X)\n",
        "        # fourth hidden layer\n",
        "        X = self.hidden3(X)\n",
        "        X = self.batchnorm3(X)\n",
        "        X = self.act3(X)\n",
        "        # fifth hidden layer\n",
        "        X = self.hidden4(X)\n",
        "        X = self.batchnorm4(X)\n",
        "        X = self.act4(X)\n",
        "        # X = self.dropout(X)\n",
        "        # # output layer\n",
        "        # X = self.hidden3(X)\n",
        "        # X = self.act3(X)\n",
        "        return X\n",
        "\n",
        "# shared model definition\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs=NUM+DIFFERECE_COL):\n",
        "        super(MLP, self).__init__()\n",
        "        # # input to very beginning hidden layer\n",
        "        self.hidden = Linear(n_inputs, 128)\n",
        "        kaiming_uniform_(self.hidden.weight, nonlinearity='relu')\n",
        "        self.act = ReLU()\n",
        "        # input to beginning hidden layer\n",
        "        self.hidden0 = Linear(128, 256)\n",
        "        kaiming_uniform_(self.hidden0.weight, nonlinearity='relu')\n",
        "        self.act0 = ReLU()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(256, 128)\n",
        "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(128, 64)\n",
        "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "        self.act2 = ReLU()\n",
        "        # # third hidden layer and output\n",
        "        # self.hidden3 = Linear(64, 6)\n",
        "        # xavier_uniform_(self.hidden3.weight)\n",
        "        # self.act3 = Softmax(dim=1)\n",
        "        self.dropout = Dropout(p=0.5)\n",
        "        self.batchnorm = BatchNorm1d(128)\n",
        "        self.batchnorm0 = BatchNorm1d(256)\n",
        "        self.batchnorm1 = BatchNorm1d(128)\n",
        "        self.batchnorm2 = BatchNorm1d(64)\n",
        "\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to very first hidden layer\n",
        "        X = self.hidden(X)\n",
        "        X = self.batchnorm(X)\n",
        "        X = self.act(X)\n",
        "        X = self.dropout(X)\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden0(X)\n",
        "        X = self.batchnorm0(X)\n",
        "        X = self.act0(X)\n",
        "        X = self.dropout(X)\n",
        "        # input to second hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.batchnorm1(X)\n",
        "        X = self.act1(X)\n",
        "        X = self.dropout(X)\n",
        "        # third hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.batchnorm2(X)\n",
        "        X = self.act2(X)\n",
        "        # X = self.dropout(X)\n",
        "        # # output layer\n",
        "        # X = self.hidden3(X)\n",
        "        # X = self.act3(X)\n",
        "        return X\n",
        "\n",
        "# prepare the dataset - random split within a dataset\n",
        "def prepare_data(X2_train, Y2_train, X2_test, Y2_test):\n",
        "    # load the train dataset\n",
        "    train = CSVDataset(X2_train, Y2_train)\n",
        "    # load the test dataset\n",
        "    test = CSVDataset(X2_test, Y2_test)\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "aggre_losses = []\n",
        "aggre_train_acc = []\n",
        "aggre_test_acc = []\n",
        "aggre_train_tgt_acc = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVEv03qKzI4y",
        "colab_type": "text"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBA5BSIzHns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "def train_model(train_src, train_tgt, model):\n",
        "    # define the optimization\n",
        "    criterion = CrossEntropyLoss()\n",
        "    l2loss = MSELoss()\n",
        "    # optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "    optimizer = RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "    # enumerate epochs\n",
        "    j = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        j += 1\n",
        "        # enumerate mini batches of src domain and target domain\n",
        "        train_steps = min(len(train_src), len(train_tgt))\n",
        "        print(\"train_steps:\", train_steps)\n",
        "        iter_src = iter(train_src)\n",
        "        iter_tgt = iter(train_tgt)\n",
        "\n",
        "        epoch_loss = 0\n",
        "        for i in range(train_steps):\n",
        "            # inputs, targets = train_data.next()\n",
        "            # load the src and target training data\n",
        "            src_data, src_label = iter_src.next()\n",
        "            tgt_data, tgt_label = iter_tgt.next()\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            # yhat = model(inputs)\n",
        "            src_out, tgt_out, dm_out = model(src_data, tgt_data)\n",
        "\n",
        "            # calculate loss\n",
        "            # loss = criterion(yhat, targets)\n",
        "            # epoch_loss = loss\n",
        "            loss_classifier = criterion(src_out, src_label)\n",
        "            loss_coral = CORAL(src_out, tgt_out)\n",
        "            # loss_l2 = l2loss(src_out, tgt_out)\n",
        "            loss_l2 = l2loss(dm_out, src_data)\n",
        "\n",
        "            sum_loss = lambda_ * loss_coral + loss_classifier + loss_l2 * lambda_l2\n",
        "            #sum_loss = lambda_ * loss_coral + loss_classifier * loss_l2\n",
        "\n",
        "            epoch_loss += sum_loss\n",
        "\n",
        "            # credit assignment\n",
        "            sum_loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Train Epoch: {:2d} [{:2d}/{:2d}]\\t'\n",
        "              'Lambda: {:.4f}, Class: {:.6f}, CORAL: {:.6f}, l2_loss: {:.6f}, Total_Loss: {:.6f}'.format(\n",
        "            epoch,\n",
        "            i + 1,\n",
        "            train_steps,\n",
        "            lambda_,\n",
        "            loss_classifier.item(),\n",
        "            loss_coral.item(),\n",
        "            loss_l2.item(),\n",
        "            sum_loss.item()\n",
        "        ))\n",
        "        print('Train ith Epoch %d result:' % epoch)\n",
        "        # calculate train src accuracy\n",
        "        train_acc = evaluate_source(train_src, model)\n",
        "        aggre_train_acc.append(train_acc)\n",
        "        print('train_acc: %.3f' % train_acc)\n",
        "\n",
        "        # calculate train tgt accuracy\n",
        "        train_tgt_acc = evaluate_model(train_tgt, model)\n",
        "        aggre_train_tgt_acc.append(train_tgt_acc)\n",
        "        print('train_tgt_acc: %.3f' % train_tgt_acc)\n",
        "\n",
        "        # calculate validate accuracy\n",
        "        test_acc = evaluate_model(test_tgt, model)\n",
        "        aggre_test_acc.append(test_acc)\n",
        "        print('test_acc: %.3f' % test_acc)\n",
        "\n",
        "        epoch_loss = epoch_loss / train_steps\n",
        "        aggre_losses.append(epoch_loss)\n",
        "        print(f'epoch: {j:3} loss: {epoch_loss.item():6.4f}')\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_source(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    test_steps = len(test_dl)\n",
        "    iter_test = iter(test_dl)\n",
        "    # for i, (inputs, targets) in enumerate(test_dl):\n",
        "    for i in range(test_steps):\n",
        "        # evaluate the model on the test set\n",
        "        tgt_data, targets = iter_test.next()\n",
        "        # temp = torch.zeros((tgt_data.shape[0], DIFFERECE_COL))\n",
        "        # tmp_data = torch.cat((tgt_data, temp), 1)\n",
        "        with torch.no_grad():\n",
        "          yhat, _, _ = model(tgt_data, tgt_data[:,0:26])\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        # convert to class labels\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        # reshape for stacking\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    test_steps = len(test_dl)\n",
        "    iter_test = iter(test_dl)\n",
        "    # for i, (inputs, targets) in enumerate(test_dl):\n",
        "    for i in range(test_steps):\n",
        "        # evaluate the model on the test set\n",
        "        tgt_data, targets = iter_test.next()\n",
        "        temp = torch.zeros((tgt_data.shape[0], DIFFERECE_COL))\n",
        "        tmp_data = torch.cat((tgt_data, temp), 1)\n",
        "        with torch.no_grad():\n",
        "          # yhat, _ = model(tgt_data, tgt_data)\n",
        "          yhat, _, _ = model(tmp_data, tgt_data)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        # convert to class labels\n",
        "        yhat = argmax(yhat, axis=1)\n",
        "        # reshape for stacking\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n",
        "\n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys9wJpm1yUY_",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O2OqLbWyTWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_s = x_train_src\n",
        "Y_s = y_train\n",
        "\n",
        "X_t = x_train_pt\n",
        "Y_t = Y_s\n",
        "\n",
        "X_s_test = X_s_test\n",
        "Y_s_test = Y_s_test\n",
        "X_t_test = x_test_pt_test\n",
        "Y_t_test = Y_s_test\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "# y_valid = keras.utils.to_categorical(y_valid-1, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
        "\n",
        "# prepare the data\n",
        "train_src, test_src = prepare_data(X_s, Y_s, X_s_test, Y_s_test)\n",
        "train_tgt, test_tgt = prepare_data(X_t, Y_t, X_t_test, Y_t_test)\n",
        "\n",
        "# print(\"train_dl\")\n",
        "# print(train_dl)\n",
        "# print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "# define the network\n",
        "# model = MLP(20)\n",
        "model = Deep_coral(num_classes=6)\n",
        "# train the model\n",
        "train_model(train_src, train_tgt, model)\n",
        "# train_model(train_tgt, train_src, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02qaqsEtx9YX",
        "colab_type": "text"
      },
      "source": [
        " ## Plot and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRb0Hd27x4mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model\n",
        "# acc = evaluate_model(test_tgt, model)\n",
        "acc = evaluate_model(test_tgt, model)\n",
        "\n",
        "print('Accuracy: %.3f' % acc)\n",
        "# make a single prediction\n",
        "# row = [5.1,3.5,1.4,0.2]\n",
        "# yhat = predict(row, model)\n",
        "# print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n",
        "\n",
        "#plot the loss\n",
        "plt.figure()\n",
        "plt.plot(range(n_epochs), aggre_losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch');\n",
        "\n",
        "#plot the train accuracy\n",
        "plt.figure()\n",
        "plt.plot(range(n_epochs), aggre_train_acc, '-', label='train src')\n",
        "plt.plot(range(n_epochs), aggre_train_tgt_acc, '-', label='train tgt')\n",
        "plt.plot(range(n_epochs), aggre_test_acc, '-', label='test tgt')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Clasification accuracy')\n",
        "plt.legend()\n",
        "plt.gcf().set_size_inches(14, 4)\n",
        "\n",
        "\n",
        "#plot the combined\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(n_epochs), aggre_losses, '-')\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(n_epochs), aggre_train_acc, '-', label='train src')\n",
        "plt.plot(range(n_epochs), aggre_train_tgt_acc, '-', label='train tgt')\n",
        "plt.plot(range(n_epochs), aggre_test_acc, '-', label='test tgt')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Clasification accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.gcf().set_size_inches(14, 4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4axPplS_9cCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train Epoch:  0 [241/241]\tLambda: 0.0001, Class: 0.040291, CORAL: 0.036961, l2_loss: 0.863722, Total_Loss: 0.048932\n",
        "Train ith Epoch 0 result:\n",
        "train_acc: 0.985\n",
        "train_tgt_acc: 0.262\n",
        "test_acc: 0.303\n",
        "epoch:   1 loss: 0.1650\n",
        "train_steps: 241\n",
        "Train Epoch:  1 [241/241]\tLambda: 0.0001, Class: 0.020836, CORAL: 0.075486, l2_loss: 0.891862, Total_Loss: 0.029762\n",
        "Train ith Epoch 1 result:\n",
        "train_acc: 0.993\n",
        "train_tgt_acc: 0.255\n",
        "test_acc: 0.294\n",
        "epoch:   2 loss: 0.0457\n",
        "train_steps: 241\n",
        "Train Epoch:  2 [241/241]\tLambda: 0.0001, Class: 0.010436, CORAL: 0.042576, l2_loss: 0.943396, Total_Loss: 0.019874\n",
        "Train ith Epoch 2 result:\n",
        "train_acc: 0.996\n",
        "train_tgt_acc: 0.257\n",
        "test_acc: 0.298\n",
        "epoch:   3 loss: 0.0295\n",
        "train_steps: 241\n",
        "Train Epoch:  3 [241/241]\tLambda: 0.0001, Class: 0.011951, CORAL: 0.030750, l2_loss: 0.927052, Total_Loss: 0.021225\n",
        "Train ith Epoch 3 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.260\n",
        "test_acc: 0.303\n",
        "epoch:   4 loss: 0.0223\n",
        "train_steps: 241\n",
        "Train Epoch:  4 [241/241]\tLambda: 0.0001, Class: 0.008217, CORAL: 0.075173, l2_loss: 0.895514, Total_Loss: 0.017180\n",
        "Train ith Epoch 4 result:\n",
        "train_acc: 0.998\n",
        "train_tgt_acc: 0.260\n",
        "test_acc: 0.298\n",
        "epoch:   5 loss: 0.0191\n",
        "train_steps: 241"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJ3bgFQd9Zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train Epoch:  0 [241/241]\tLambda: 0.0000, Class: 0.567423, CORAL: 0.004632, Total_Loss: 0.567423\n",
        "Train ith Epoch 0 result:\n",
        "train_acc: 0.834\n",
        "train_tgt_acc: 0.650\n",
        "test_acc: 0.627\n",
        "epoch:   1 loss: 0.9673\n",
        "train_steps: 241\n",
        "Train Epoch:  1 [241/241]\tLambda: 0.0000, Class: 0.347108, CORAL: 0.007994, Total_Loss: 0.347108\n",
        "Train ith Epoch 1 result:\n",
        "train_acc: 0.907\n",
        "train_tgt_acc: 0.674\n",
        "test_acc: 0.652\n",
        "epoch:   2 loss: 0.4377\n",
        "train_steps: 241\n",
        "Train Epoch:  2 [241/241]\tLambda: 0.0000, Class: 0.207159, CORAL: 0.021348, Total_Loss: 0.207159\n",
        "Train ith Epoch 2 result:\n",
        "train_acc: 0.939\n",
        "train_tgt_acc: 0.682\n",
        "test_acc: 0.660\n",
        "epoch:   3 loss: 0.2631\n",
        "train_steps: 241\n",
        "Train Epoch:  3 [241/241]\tLambda: 0.0000, Class: 0.161186, CORAL: 0.021150, Total_Loss: 0.161186\n",
        "Train ith Epoch 3 result:\n",
        "train_acc: 0.955\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.664\n",
        "epoch:   4 loss: 0.1790\n",
        "train_steps: 241\n",
        "Train Epoch:  4 [241/241]\tLambda: 0.0000, Class: 0.118633, CORAL: 0.031945, Total_Loss: 0.118633\n",
        "Train ith Epoch 4 result:\n",
        "train_acc: 0.965\n",
        "train_tgt_acc: 0.691\n",
        "test_acc: 0.669\n",
        "epoch:   5 loss: 0.1322\n",
        "train_steps: 241\n",
        "Train Epoch:  5 [241/241]\tLambda: 0.0000, Class: 0.074903, CORAL: 0.041149, Total_Loss: 0.074903\n",
        "Train ith Epoch 5 result:\n",
        "train_acc: 0.971\n",
        "train_tgt_acc: 0.694\n",
        "test_acc: 0.673\n",
        "epoch:   6 loss: 0.1043\n",
        "train_steps: 241\n",
        "Train Epoch:  6 [241/241]\tLambda: 0.0000, Class: 0.078246, CORAL: 0.049385, Total_Loss: 0.078246\n",
        "Train ith Epoch 6 result:\n",
        "train_acc: 0.976\n",
        "train_tgt_acc: 0.695\n",
        "test_acc: 0.673\n",
        "epoch:   7 loss: 0.0863\n",
        "train_steps: 241\n",
        "Train Epoch:  7 [241/241]\tLambda: 0.0000, Class: 0.071134, CORAL: 0.057749, Total_Loss: 0.071135\n",
        "Train ith Epoch 7 result:\n",
        "train_acc: 0.979\n",
        "train_tgt_acc: 0.697\n",
        "test_acc: 0.674\n",
        "epoch:   8 loss: 0.0730\n",
        "train_steps: 241\n",
        "Train Epoch:  8 [241/241]\tLambda: 0.0000, Class: 0.082524, CORAL: 0.042208, Total_Loss: 0.082524\n",
        "Train ith Epoch 8 result:\n",
        "train_acc: 0.982\n",
        "train_tgt_acc: 0.698\n",
        "test_acc: 0.677\n",
        "epoch:   9 loss: 0.0634\n",
        "train_steps: 241\n",
        "Train Epoch:  9 [241/241]\tLambda: 0.0000, Class: 0.056301, CORAL: 0.082851, Total_Loss: 0.056302\n",
        "Train ith Epoch 9 result:\n",
        "train_acc: 0.984\n",
        "train_tgt_acc: 0.699\n",
        "test_acc: 0.679\n",
        "epoch:  10 loss: 0.0560\n",
        "train_steps: 241"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdhzBAldzeva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "1d7de72c-81b0-4225-a439-203f7fc8562d"
      },
      "source": [
        "data_test = np.load('/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/test_138_day.npz')\n",
        "\n",
        "passive =1\n",
        "\n",
        "#load common data\n",
        "latlon_test = data_test['latlon']\n",
        "iff_test = data_test['iff']\n",
        "\n",
        "# if passive ==1:\n",
        "x_t_test = data_test['viirs']\n",
        "y_t_test = data_test['label']\n",
        "# else:\n",
        "x_s_test = data_test['calipso']\n",
        "y_s_test = data_test['label']\n",
        "    \n",
        "inds_test,vals_test = np.where(y_t_test>0)\n",
        "\n",
        "# process common data\n",
        "Latlon_test = latlon_test[inds_test]\n",
        "Iff_test = iff_test[inds_test]\n",
        "\n",
        "Y_t_test = y_t_test[inds_test]\n",
        "X_t_test = x_t_test[inds_test]\n",
        "\n",
        "Y_s_test = y_s_test[inds_test]\n",
        "X_s_test = x_s_test[inds_test]\n",
        "\n",
        "# 0 =< SZA <= 83\n",
        "print('original X_t_test: ', X_t_test.shape)\n",
        "rows_test = np.where((X_t_test[:,0] >= 0) & (X_t_test[:,0] <= 83) & (X_t_test[:,15] > 100) & (X_t_test[:,15] < 400) & (X_t_test[:,16] > 100) & (X_t_test[:,16] < 400) & (X_t_test[:,17] > 100) & (X_t_test[:,17] < 400) & (X_t_test[:,18] > 100) & (X_t_test[:,18] < 400) & (X_t_test[:,19] > 100) & (X_t_test[:,19] < 400) & (X_t_test[:,10] > 0))\n",
        "print(\"rows_test:\", rows_test)\n",
        "print(\"rows_test.shape:\", len(rows_test))\n",
        "\n",
        "Latlon_test = Latlon_test[rows_test]\n",
        "Iff_test = Iff_test[rows_test]\n",
        "\n",
        "Y_t_test = Y_t_test[rows_test]\n",
        "X_t_test = X_t_test[rows_test]\n",
        "\n",
        "Y_s_test = Y_s_test[rows_test]\n",
        "X_s_test = X_s_test[rows_test]\n",
        "\n",
        "X_s_test = np.nan_to_num(X_s_test)\n",
        "X_t_test = np.nan_to_num(X_t_test)\n",
        "\n",
        "print('after SZA X_t_test: ', X_t_test.shape)\n",
        "print('after SZA X_s_test: ', X_s_test.shape)\n",
        "\n",
        "# pca = decomposition.PCA(n_components=20)\n",
        "# pca.fit(X_s_test)\n",
        "# X_s_test = pca.transform(X_s_test)\n",
        "# print (X_s_test.shape)\n",
        "\n",
        "#concanate common data\n",
        "# X_t_test = np.concatenate((X_t_test, Latlon_test, Iff_test), axis=1)\n",
        "X_s_test = np.concatenate((X_s_test, Latlon_test, Iff_test), axis=1)\n",
        "\n",
        "print (X_s_test.shape)\n",
        "print (X_t_test.shape)\n",
        "\n",
        "X_test=np.concatenate((X_t_test, X_s_test), axis=1)\n",
        "\n",
        "# scaler_t = StandardScaler()\n",
        "# scaler_t.fit(X_t_test)\n",
        "# X_t_test = scaler_t.transform(X_t_test)\n",
        "\n",
        "# scaler_s = StandardScaler()\n",
        "# scaler_s.fit(X_s_test)\n",
        "# X_s_test= scaler_s.transform(X_s_test)\n",
        "\n",
        "x_test2=sc_X.transform(X_test)\n",
        "\n",
        "# x_train_v = x_train[:, 0:20]\n",
        "# x_train_c = x_train[:, 20:45]\n",
        "# x_train_comm = x_train[:, 45:51]\n",
        "# print(x_train_v.shape)\n",
        "# print(x_train_c.shape)\n",
        "# print(x_train_comm.shape)\n",
        "\n",
        "# x_valid_v = x_valid[:, 0:20]\n",
        "# x_valid_c = x_valid[:, 20:45]\n",
        "# x_valid_comm = x_valid[:, 45:51]\n",
        "\n",
        "# print(x_valid_v.shape)\n",
        "# print(x_valid_c.shape)\n",
        "# print(x_valid_comm.shape)\n",
        "\n",
        "X_t_test = x_test2[:, 0:20]\n",
        "x_test_c2 = x_test2[:, 20:45]\n",
        "x_test_comm2 = x_test2[:, 45:51]\n",
        "\n",
        "\n",
        "# DLR imputed target domain\n",
        "# x_test_t_pt = model_reg.predict(X_t_test)\n",
        "# print(x_test_t_pt.shape)\n",
        "\n",
        "x_test_pt_test = np.concatenate((X_t_test, x_test_comm2),axis=1)\n",
        "print(x_test_pt_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8e3769e40981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/content/My Drive/Colab Notebooks/kddworkshop/fulldata/test_138_day.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpassive\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#load common data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TecWVAgxzgvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_t = x_train_pt\n",
        "Y_t = y_train\n",
        "\n",
        "# X_s_test = X_s_test\n",
        "# Y_s_test = Y_s_test\n",
        "X_t_test = x_test_pt_test\n",
        "Y_t_test = Y_s_test\n",
        "\n",
        "\n",
        "train_tgt, test_tgt = prepare_data(X_t, Y_t, X_t_test, Y_t_test)\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "acc = evaluate_model(test_tgt, model)\n",
        "print('test Accuracy: %.3f' % acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AupWGYj8kouZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train Epoch:  0 [241/241]\tLambda: 0.0000, Class: 0.540120, CORAL: 0.008095, Total_Loss: 0.540120\n",
        "Train ith Epoch 0 result:\n",
        "train_acc: 0.826\n",
        "train_tgt_acc: 0.645\n",
        "test_acc: 0.627\n",
        "epoch:   1 loss: 0.9171\n",
        "train_steps: 241\n",
        "Train Epoch:  1 [241/241]\tLambda: 0.0000, Class: 0.315844, CORAL: 0.008346, Total_Loss: 0.315844\n",
        "Train ith Epoch 1 result:\n",
        "train_acc: 0.907\n",
        "train_tgt_acc: 0.665\n",
        "test_acc: 0.647\n",
        "epoch:   2 loss: 0.4236\n",
        "train_steps: 241\n",
        "Train Epoch:  2 [241/241]\tLambda: 0.0000, Class: 0.225985, CORAL: 0.012512, Total_Loss: 0.225985\n",
        "Train ith Epoch 2 result:\n",
        "train_acc: 0.939\n",
        "train_tgt_acc: 0.673\n",
        "test_acc: 0.656\n",
        "epoch:   3 loss: 0.2570\n",
        "train_steps: 241\n",
        "Train Epoch:  3 [241/241]\tLambda: 0.0000, Class: 0.169600, CORAL: 0.027440, Total_Loss: 0.169600\n",
        "Train ith Epoch 3 result:\n",
        "train_acc: 0.955\n",
        "train_tgt_acc: 0.677\n",
        "test_acc: 0.661\n",
        "epoch:   4 loss: 0.1745\n",
        "train_steps: 241\n",
        "Train Epoch:  4 [241/241]\tLambda: 0.0000, Class: 0.116169, CORAL: 0.026667, Total_Loss: 0.116169\n",
        "Train ith Epoch 4 result:\n",
        "train_acc: 0.966\n",
        "train_tgt_acc: 0.679\n",
        "test_acc: 0.661\n",
        "epoch:   5 loss: 0.1299\n",
        "train_steps: 241\n",
        "Train Epoch:  5 [241/241]\tLambda: 0.0000, Class: 0.096455, CORAL: 0.041388, Total_Loss: 0.096456\n",
        "Train ith Epoch 5 result:\n",
        "train_acc: 0.972\n",
        "train_tgt_acc: 0.682\n",
        "test_acc: 0.662\n",
        "epoch:   6 loss: 0.1023\n",
        "train_steps: 241\n",
        "Train Epoch:  6 [241/241]\tLambda: 0.0000, Class: 0.077469, CORAL: 0.059802, Total_Loss: 0.077469\n",
        "Train ith Epoch 6 result:\n",
        "train_acc: 0.976\n",
        "train_tgt_acc: 0.683\n",
        "test_acc: 0.665\n",
        "epoch:   7 loss: 0.0853\n",
        "train_steps: 241\n",
        "Train Epoch:  7 [241/241]\tLambda: 0.0000, Class: 0.060783, CORAL: 0.078827, Total_Loss: 0.060784\n",
        "Train ith Epoch 7 result:\n",
        "train_acc: 0.980\n",
        "train_tgt_acc: 0.684\n",
        "test_acc: 0.664\n",
        "epoch:   8 loss: 0.0713\n",
        "train_steps: 241\n",
        "Train Epoch:  8 [241/241]\tLambda: 0.0000, Class: 0.049636, CORAL: 0.098868, Total_Loss: 0.049637\n",
        "Train ith Epoch 8 result:\n",
        "train_acc: 0.982\n",
        "train_tgt_acc: 0.684\n",
        "test_acc: 0.667\n",
        "epoch:   9 loss: 0.0625\n",
        "train_steps: 241\n",
        "Train Epoch:  9 [241/241]\tLambda: 0.0000, Class: 0.060630, CORAL: 0.108205, Total_Loss: 0.060631\n",
        "Train ith Epoch 9 result:\n",
        "train_acc: 0.984\n",
        "train_tgt_acc: 0.685\n",
        "test_acc: 0.666\n",
        "epoch:  10 loss: 0.0552\n",
        "train_steps: 241\n",
        "Train Epoch: 10 [241/241]\tLambda: 0.0000, Class: 0.036247, CORAL: 0.216891, Total_Loss: 0.036249\n",
        "Train ith Epoch 10 result:\n",
        "train_acc: 0.986\n",
        "train_tgt_acc: 0.686\n",
        "test_acc: 0.667\n",
        "epoch:  11 loss: 0.0491\n",
        "train_steps: 241\n",
        "Train Epoch: 11 [241/241]\tLambda: 0.0000, Class: 0.052475, CORAL: 0.153238, Total_Loss: 0.052477\n",
        "Train ith Epoch 11 result:\n",
        "train_acc: 0.987\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.668\n",
        "epoch:  12 loss: 0.0443\n",
        "train_steps: 241\n",
        "Train Epoch: 12 [241/241]\tLambda: 0.0000, Class: 0.032792, CORAL: 0.170662, Total_Loss: 0.032794\n",
        "Train ith Epoch 12 result:\n",
        "train_acc: 0.988\n",
        "train_tgt_acc: 0.686\n",
        "test_acc: 0.668\n",
        "epoch:  13 loss: 0.0399\n",
        "train_steps: 241\n",
        "Train Epoch: 13 [241/241]\tLambda: 0.0000, Class: 0.033418, CORAL: 0.195935, Total_Loss: 0.033420\n",
        "Train ith Epoch 13 result:\n",
        "train_acc: 0.990\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.668\n",
        "epoch:  14 loss: 0.0361\n",
        "train_steps: 241\n",
        "Train Epoch: 14 [241/241]\tLambda: 0.0000, Class: 0.052977, CORAL: 0.282034, Total_Loss: 0.052980\n",
        "Train ith Epoch 14 result:\n",
        "train_acc: 0.990\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  15 loss: 0.0333\n",
        "train_steps: 241\n",
        "Train Epoch: 15 [241/241]\tLambda: 0.0000, Class: 0.040439, CORAL: 0.303759, Total_Loss: 0.040442\n",
        "Train ith Epoch 15 result:\n",
        "train_acc: 0.991\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.668\n",
        "epoch:  16 loss: 0.0312\n",
        "train_steps: 241\n",
        "Train Epoch: 16 [241/241]\tLambda: 0.0000, Class: 0.025769, CORAL: 0.256855, Total_Loss: 0.025771\n",
        "Train ith Epoch 16 result:\n",
        "train_acc: 0.992\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.668\n",
        "epoch:  17 loss: 0.0287\n",
        "train_steps: 241\n",
        "Train Epoch: 17 [241/241]\tLambda: 0.0000, Class: 0.023763, CORAL: 0.154918, Total_Loss: 0.023765\n",
        "Train ith Epoch 17 result:\n",
        "train_acc: 0.992\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  18 loss: 0.0264\n",
        "train_steps: 241\n",
        "Train Epoch: 18 [241/241]\tLambda: 0.0000, Class: 0.032427, CORAL: 0.256005, Total_Loss: 0.032430\n",
        "Train ith Epoch 18 result:\n",
        "train_acc: 0.993\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  19 loss: 0.0249\n",
        "train_steps: 241\n",
        "Train Epoch: 19 [241/241]\tLambda: 0.0000, Class: 0.025798, CORAL: 0.536587, Total_Loss: 0.025804\n",
        "Train ith Epoch 19 result:\n",
        "train_acc: 0.994\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.669\n",
        "epoch:  20 loss: 0.0232\n",
        "train_steps: 241\n",
        "Train Epoch: 20 [241/241]\tLambda: 0.0000, Class: 0.021837, CORAL: 0.228272, Total_Loss: 0.021840\n",
        "Train ith Epoch 20 result:\n",
        "train_acc: 0.994\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  21 loss: 0.0218\n",
        "train_steps: 241\n",
        "Train Epoch: 21 [241/241]\tLambda: 0.0000, Class: 0.029590, CORAL: 0.185536, Total_Loss: 0.029592\n",
        "Train ith Epoch 21 result:\n",
        "train_acc: 0.994\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  22 loss: 0.0203\n",
        "train_steps: 241\n",
        "Train Epoch: 22 [241/241]\tLambda: 0.0000, Class: 0.018301, CORAL: 0.591578, Total_Loss: 0.018307\n",
        "Train ith Epoch 22 result:\n",
        "train_acc: 0.995\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.670\n",
        "epoch:  23 loss: 0.0200\n",
        "train_steps: 241\n",
        "Train Epoch: 23 [241/241]\tLambda: 0.0000, Class: 0.028393, CORAL: 0.352721, Total_Loss: 0.028397\n",
        "Train ith Epoch 23 result:\n",
        "train_acc: 0.995\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.668\n",
        "epoch:  24 loss: 0.0185\n",
        "train_steps: 241\n",
        "Train Epoch: 24 [241/241]\tLambda: 0.0000, Class: 0.013947, CORAL: 0.357974, Total_Loss: 0.013951\n",
        "Train ith Epoch 24 result:\n",
        "train_acc: 0.995\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.670\n",
        "epoch:  25 loss: 0.0169\n",
        "train_steps: 241\n",
        "Train Epoch: 25 [241/241]\tLambda: 0.0000, Class: 0.022838, CORAL: 0.427041, Total_Loss: 0.022842\n",
        "Train ith Epoch 25 result:\n",
        "train_acc: 0.995\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.671\n",
        "epoch:  26 loss: 0.0166\n",
        "train_steps: 241\n",
        "Train Epoch: 26 [241/241]\tLambda: 0.0000, Class: 0.017920, CORAL: 0.434516, Total_Loss: 0.017925\n",
        "Train ith Epoch 26 result:\n",
        "train_acc: 0.996\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  27 loss: 0.0157\n",
        "train_steps: 241\n",
        "Train Epoch: 27 [241/241]\tLambda: 0.0000, Class: 0.011958, CORAL: 0.463105, Total_Loss: 0.011962\n",
        "Train ith Epoch 27 result:\n",
        "train_acc: 0.996\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  28 loss: 0.0149\n",
        "train_steps: 241\n",
        "Train Epoch: 28 [241/241]\tLambda: 0.0000, Class: 0.010395, CORAL: 0.498058, Total_Loss: 0.010400\n",
        "Train ith Epoch 28 result:\n",
        "train_acc: 0.996\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  29 loss: 0.0146\n",
        "train_steps: 241\n",
        "Train Epoch: 29 [241/241]\tLambda: 0.0000, Class: 0.018954, CORAL: 0.365303, Total_Loss: 0.018957\n",
        "Train ith Epoch 29 result:\n",
        "train_acc: 0.996\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  30 loss: 0.0138\n",
        "train_steps: 241\n",
        "Train Epoch: 30 [241/241]\tLambda: 0.0000, Class: 0.014405, CORAL: 0.583118, Total_Loss: 0.014411\n",
        "Train ith Epoch 30 result:\n",
        "train_acc: 0.996\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  31 loss: 0.0135\n",
        "train_steps: 241\n",
        "Train Epoch: 31 [241/241]\tLambda: 0.0000, Class: 0.004955, CORAL: 0.824178, Total_Loss: 0.004963\n",
        "Train ith Epoch 31 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  32 loss: 0.0126\n",
        "train_steps: 241\n",
        "Train Epoch: 32 [241/241]\tLambda: 0.0000, Class: 0.020540, CORAL: 0.628661, Total_Loss: 0.020546\n",
        "Train ith Epoch 32 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.671\n",
        "epoch:  33 loss: 0.0119\n",
        "train_steps: 241\n",
        "Train Epoch: 33 [241/241]\tLambda: 0.0000, Class: 0.014973, CORAL: 0.681811, Total_Loss: 0.014980\n",
        "Train ith Epoch 33 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.688\n",
        "test_acc: 0.670\n",
        "epoch:  34 loss: 0.0117\n",
        "train_steps: 241\n",
        "Train Epoch: 34 [241/241]\tLambda: 0.0000, Class: 0.013220, CORAL: 0.703681, Total_Loss: 0.013228\n",
        "Train ith Epoch 34 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  35 loss: 0.0111\n",
        "train_steps: 241\n",
        "Train Epoch: 35 [241/241]\tLambda: 0.0000, Class: 0.022554, CORAL: 0.649579, Total_Loss: 0.022561\n",
        "Train ith Epoch 35 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  36 loss: 0.0110\n",
        "train_steps: 241\n",
        "Train Epoch: 36 [241/241]\tLambda: 0.0000, Class: 0.006245, CORAL: 0.558690, Total_Loss: 0.006251\n",
        "Train ith Epoch 36 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  37 loss: 0.0102\n",
        "train_steps: 241\n",
        "Train Epoch: 37 [241/241]\tLambda: 0.0000, Class: 0.013176, CORAL: 0.523414, Total_Loss: 0.013182\n",
        "Train ith Epoch 37 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  38 loss: 0.0097\n",
        "train_steps: 241\n",
        "Train Epoch: 38 [241/241]\tLambda: 0.0000, Class: 0.012121, CORAL: 0.510060, Total_Loss: 0.012126\n",
        "Train ith Epoch 38 result:\n",
        "train_acc: 0.997\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.669\n",
        "epoch:  39 loss: 0.0093\n",
        "train_steps: 241\n",
        "Train Epoch: 39 [241/241]\tLambda: 0.0000, Class: 0.009975, CORAL: 0.797157, Total_Loss: 0.009983\n",
        "Train ith Epoch 39 result:\n",
        "train_acc: 0.998\n",
        "train_tgt_acc: 0.687\n",
        "test_acc: 0.670\n",
        "epoch:  40 loss: "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}